{
  "layers": 8,
  "dropout": 0.04,
  "neurons": 160,
  "epochs": 300,
  "lr": 0.12,
  "patience": 35,
  "pats": 20,
  "batchsize": 100,
  "activation": "relu"
}
